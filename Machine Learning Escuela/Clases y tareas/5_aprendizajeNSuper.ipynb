{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje No Supervisado\n",
    "\n",
    "Mientras el aprendizaje supervisado busca una respuesta a una pregunta definida, el no supervisado intenta comprender lo subyacente, definir patrones sin saber qué representan.\n",
    "\n",
    "- En este caso no se conoce la bariable target.\n",
    "- Se basa en explorar la distribución interna de los datos sin un objetivo dado.\n",
    "- Permite descubrir patrones, estructuras latentes o variables ocultas, relaciones entre características y outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmos comunes\n",
    "\n",
    "#### K-Means \n",
    "\n",
    "Particiona el conjunto en k grupos, minimiza la variación de cada cluster. Parte de k centroides iniciales (al azar) e iterativamente asigna cada ounto al centroide más cercano, recalcula la posición de los centroides hasta la convergencia.\n",
    "Se asumen grupos esféricos y de tamaño similar. Funciona correctamente con datos numéricos y métricas como la distancia euclídea.\n",
    "\n",
    "Se aconseja emplear cuando se tiene una idea de cuántos grupos se desean y son conformados en estructuras algo compactas. No maneja bien formas complejas o densidades variables, pero es escalable a grandes conjuntos de datos.\n",
    "\n",
    "#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "Agrupa puntos que están en zonas de alta densidad, separadas por zonas de menor densidad. Un punto núcleo es aquel con al menos un número mínimo de vecinos dentro de un radio específico. Los puntos en su vecindad directa foraman parte del mismo cluster, los que no encajan son ruido.\n",
    "\n",
    "Detecta clusteres de foram arbitraría y sin un número específo en cuanto a la cantidad e identifica los outliers como ruido. No agrupa bien los conjuntos si hay grandes diferencias de densidad. Se suele emplear cuando los datos deben tener formas de clusteres complejas, de distintas densidades y sin saber cuántos grupos pueden ser. También es útil para identificar anomalías o ruidos.\n",
    "\n",
    "#### PCA (Principal Component Analysis)\n",
    "\n",
    "Halla combinaciones lineales de las variables originales que capturan la mayor parte de la varianza de los datos. Estos componentes principales están ordenadas por la cantidad de varianza que explican.\n",
    "\n",
    "Es útil para reducir la dimensionalidad y eliminar el ruido. Permite obtener una visión global de los datos, visualizar en 2d o 3d datos muy complejos y preprocesar antes de aplicar otros algoritmos de clustering. Como desventaja solo captura relaciones lineales entre variables y las componenetes principales pueden ser difíciles de interpretar biológicamente\n",
    "\n",
    "#### UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "Los datos residen en una variedad (manifold) de menor dimensión dentro del espacio original. UMAP construye un gráfico local y lo proyecta a una dimensión más baja (2d o 3d) conservando las relaciones de vecindad.\n",
    "\n",
    "Mantiene la estructura local y global con más éxito que t-SNE. Necesita ajustar parámetros como n_neighbors y min_dist. Los resultados dependen de la inicialización. Es útil en la visualización de datos muy complejos conservando relaciones importantes entre puntos, especialmente en análisis single-cell en bioinformática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso de aprendizaje no supervisado\n",
    "\n",
    "- Recopilación de datos\n",
    "- Contextualizar variables\n",
    "- Evaluar calidad de los datos (valores faltantes, inconsistencias, ruido)\n",
    "- Limpieza de datos (imputación, eliminación)\n",
    "- Transformación y normalización (estandarización, normalización y codificación de variables categóricas)\n",
    "- Selección y extracción de características(reducción de dimension con PCA o UMAP y seleccionar variables más relevantes)\n",
    "\n",
    "Algoritmo de modelo\n",
    "\n",
    "- Análisis de la estructura esperada (esféricos a K-Means, densidad variable a DBSCAN o Mean Shift, visualización menor dimensión PCA o UMAP, subgrupos probabilísticos con distribución gaussiana, Gaussian Mixture Models)\n",
    "- Ajuste de hiperparámetros (k, eps y min_samples, número de componentes, se experimenta con diferentes configuraciones)\n",
    "- Ejecución de algoritmo, entrenamiento con datos preprocesados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de la calidad de los resultados\n",
    "\n",
    "- Métricas Internas (índice de Silhouette, Calinski-Harabasz, Davies Bouldin para clusters...comparar métricas)\n",
    "- Métricas externas si hay etiquetas posteriores o experticia(ajustar Ran Index, Mutual Information...correlación con subtipos de una enfermedad)\n",
    "- Visualización de resultados(representación en 2d y 3d con PCA, t-SNE o UMAP para observar separación de grupos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación y validación\n",
    "\n",
    "- Interpretación de los patrones descubiertos\n",
    "- Validación con expertos\n",
    "\n",
    "### Aplicaciones de ANS\n",
    "\n",
    "- Agrupamiento de genes\n",
    "- Análisis de datos de secuencias masivas\n",
    "- Segmentación de imágenes cerebrales\n",
    "- Análisis de conectividad neuronal"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
