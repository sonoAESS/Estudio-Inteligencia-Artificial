{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingeniería de atributos\n",
    "Consiste en crear, seleccionar o transformar el dataset para mejorar el rendimiento del modeo. Escoger los atributos más relevantes para una tarea.\n",
    "\n",
    "Grados de dependencia de métodos con modelo:\n",
    "- Filtros: independendientes, se basan solo en estadísticas generales de los datos. (Selección basada en Correlación o CFS, Filtro basado en consistencia, ganancia de información y Relief)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtros\n",
    "- CFS selecciona atributos según la correlación con la variable objetivo y baja correlación entre sí para reducir redundancia. Por ejemplo relacionado con una enfermedad podemos tener en cuenta valores como el colesterol o la presión arterial que se relacionan con la enfermedad y entre sí.\n",
    "- Basado en consistencia: selecciona subconjuntos de atributos consistentes con la variable onjetivo, garantiza que el conjunto mantenga distinciones necesarias. Es útil para la clasificación de textos, donde términos específicos pueden ser consistentes con clasificaciones como emergencia o atención médica en un modelo para clasificar reportes médicos. Descarta otros términos, lo que reduce la dimensión de forma eficiente.\n",
    "- Ganancia de información: mide el aporte de cada atributo en la clasificación, usando información mutua entre cada atributo y la clase. Útil en clasificación binaria, como detección de fraudes financieros, para identificar atributos como ubicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection con sklearn.feature_selection\n",
    "Este proceso consiste en identificar y seleccionar subconjuntos de variables o características relevantes en un conjunto de datos para mejorar el rendimiento del modelo.\n",
    "- Se reduce la dimensión al eliminar características irrelevantes o redundantes, una gran dimensión puede afectar al modelo\n",
    "- Mejora el rendimiento, características relevantes equivale a mayor precisión y robustez para generalizar a nuevos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminación de características con poca varianza\n",
    "VarianceThreshold es un método para eliminar características con poca variabilidad. Se usa en el preprocesamiento de datos para simplificar conjuntos de datos y mejorar la eficiencia del modelo.\n",
    "- Por defecto elimina las características con varianza 0, es decir, con mismo valor en todas las muestras\n",
    "-  Se puede especificar un umbral de varianza para eliminar características con varianza menor que este umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#caracteristicas booleanas(variables aleatorias de Bernoulli)\n",
    "#eliminar caracteristicas que sean 1 o 0 en mas del 80% de la muestra\n",
    "#quieres que existe al menos un 20% distinto\n",
    "# Var[X]=p(1-p)\n",
    "# Umbral=0.8*(1-0.8)=0.16\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X=[[0,0,1],[0,1,0],[1,0,0],[0,1,1],[0,1,0],[0,1,1]]\n",
    "sel=VarianceThreshold(threshold=(0.8*(1-0.8)))\n",
    "sel.fit_transform(X)\n",
    "#aca se elimina la primera columna, pues 5 de 6 son 0, es decir la probabilidad es mayor a 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate feature selection\n",
    "Consiste en identificar  razgos relevantes con pruebas estadísticas univariadas. \n",
    "- Se evalúa cada variable de forma independiente, solo en relación con el resultado a predecir\n",
    "- Se usan diversas pruebas, F-test para diferencias entre las medias de dos o más grupos, Chi-cuadrado para independencia entre variables categóricas y ANOVA para comparar medias diferentes entre grupos\n",
    "- Selección de características, SelectKBest elimina todas las variables excepto las mejores por una puntuación donde K es el número de características a conservar, SelectPercentile conserva un porcentaje específico de las mejores características y GenericUnivariateSelect hace una selección configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uso de F-test para obtener las dos caracteristicas mejores de un dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X, y=load_iris(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new=SelectKBest(f_classif,k=2).fit_transform(X,y)\n",
    "X_new.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
